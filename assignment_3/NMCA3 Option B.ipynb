{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57057a8",
   "metadata": {},
   "source": [
    "# Neuromorphic computing assignment 3 option B\n",
    "\n",
    "### By S.W. Keemink, 2023-09-21\n",
    "\n",
    "In the previous lecture you saw that if you know the neuronal activation functions (and they are rich enough), you can make a spiking neural network do a range of input-output computations of the form $y = f(x)$, where $x$ is the network intput and $y$ is the network output. The underlying theory is the neural engineering framework (or NEF) --- used in the NENGO software.\n",
    "\n",
    "**Delivarable:**  The deliverable is a pdf export of this notebook with the requested plots and questions answered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e5ce9",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67936d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb # highly recommend installing this, it will make things much faster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e3d10",
   "metadata": {},
   "source": [
    "### System setup\n",
    "We will consider a network of $N$ integrate and fire neurons the voltage of neuron $i$ is given by\n",
    "\n",
    "$$\\frac{dv_i}{dt}(t) = -v_i + f_i x(t) + b_i,$$\n",
    "\n",
    "where $v$ is the neuron's voltage, $f_i$ are is its input weight, $x(t)$ is the input into the neuron, and $b_i$ its bias. When a threshold $T_i$ is reached, a spike is emitted, and the neuron's voltage is reset to 0.\n",
    "\n",
    "A network of such neurons can be described in vector format as:\n",
    "$$\\frac{d\\mathbf V}{dt}(t) = -\\mathbf V + \\mathbf F x(t) + \\mathbf b,$$\n",
    "\n",
    "where $\\mathbf V = [v_0, ..., v_N]^\\top$, $\\mathbf F = [f_0, ..., f_N]^\\top$, and $\\mathbf b = [b_0, ..., b_N]^\\top$. Note that one would normally also add some kind of membrane-leak time-constant, but we will leave that out to keep things a bit simpler.\n",
    "\n",
    "We will filter the emitted spiketrains by an exponential filter as follows:\n",
    "$$\\frac{dr_i}{dt}(t) = -r_i(t) + s(t),$$\n",
    "where $r_i$ are the filtered spike-trains, and $s(t)$ are the spiketrains themselves.\n",
    "\n",
    "I will provide a simple Euler simulator for a network of such neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39692a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def run_LIF_network(x, F, b, T, dt=1e-3):\n",
    "    \"\"\"Simulates a network of leaky integrate-and-fire neurons (as described above).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array (shape=nT)\n",
    "        Input signal. nT=number of timepoints.\n",
    "    F : array (shape=N)\n",
    "        Input weights. N=number of neurons.\n",
    "    b : array (shape=N)\n",
    "        Biases\n",
    "    T : array (shape=N)\n",
    "        Thresholds\n",
    "    dt : float\n",
    "        Simulation timestep\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    array\n",
    "        Array of shape (N, nT) containing the simulated voltages\n",
    "    array\n",
    "        Array of shape (N, nT) containing the spikes\n",
    "    array\n",
    "        Array of shape (N, nT) containing the time-filtered spikes\n",
    "        \n",
    "    \"\"\"\n",
    "    # recover number of timepoints\n",
    "    nT = len(x)\n",
    "    N = len(F)\n",
    "    \n",
    "    # pre-define states\n",
    "    V = np.zeros((N, nT))\n",
    "    s = np.zeros((N, nT))\n",
    "    r = np.zeros((N, nT))\n",
    "    \n",
    "    # simulate with Euler\n",
    "    for i in range(1, nT):\n",
    "        # update voltage\n",
    "        V[:, i] = V[:, i-1] + dt*(-V[:, i-1] + F*x[i-1] + b)\n",
    "        \n",
    "        # update filtered spike trains\n",
    "        r[:, i] = r[:, i-1] + dt*(-r[:, i-1]) + s[:, i-1]\n",
    "        \n",
    "        # detect spikes and reset\n",
    "        spiking_neurons = np.arange(N)[V[:, i] >= T]\n",
    "        V[spiking_neurons, i] = 0 # reset\n",
    "        s[spiking_neurons, i] = 1 # set spikes\n",
    "        \n",
    "    return V, s, r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0edc2",
   "metadata": {},
   "source": [
    "Simulation of a single neuron example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time settings\n",
    "dt = 1e-3 # simulation timestep in ms\n",
    "Tend = 10 # simulation end in ms\n",
    "times = np.arange(0, Tend, dt) # time vector stored for plotting\n",
    "nT = len(times) # number of timepoints\n",
    "\n",
    "# Network parameters\n",
    "N = 10 # number of neurons\n",
    "F = np.ones(N)*0.4 # input weights\n",
    "b = np.zeros(N)*2 # biases\n",
    "T = np.ones(N) # thresholds\n",
    "\n",
    "# Define input\n",
    "x = np.ones(nT)*5 # 1D input signal (you can define KD signals by changing the shape to (K, nT))\n",
    "\n",
    "# Simulate\n",
    "V, s, r = run_LIF_network(x, F, b, T, dt=dt)\n",
    "\n",
    "# Plot a few things (only works for the first neuron at the moment)\n",
    "plt.subplot(311)\n",
    "plt.plot(times, r[0, :])\n",
    "plt.ylabel('Filtered spikes')\n",
    "plt.subplot(312)\n",
    "plt.plot(times, s[0, :])\n",
    "plt.ylabel('Spikes')\n",
    "plt.subplot(313)\n",
    "plt.plot(times, V[0, :])\n",
    "plt.axhline(T[0], color='k', linestyle='--')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Voltage')\n",
    "plt.legend(['Voltage', 'Threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d107b",
   "metadata": {},
   "source": [
    "Note that the filtered spike train needs a moment to stabilize at a fixed value (which is the correct average value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee71d2",
   "metadata": {},
   "source": [
    "### Exercise 1: Explore input-output functions of spiking neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648a914",
   "metadata": {},
   "source": [
    "1. By running several separate trials with different inputs $x(t)$ between -5 and 5 (held fixed at a given level in each trial), show what the average input output-function is. [0.5 points]\n",
    "\n",
    "For this you can measure what the average output $r(t)$ is within the range where it is stabilized (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74805a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7445a636",
   "metadata": {},
   "source": [
    "The analytical solution for the firing rate for the above neural model is:\n",
    "\n",
    "$$r = [ (\\ln(\\frac{f_i x + b_i}{f_i + b_i - T_i}))^{-1} ]^{+} $$\n",
    "\n",
    "Which we implement below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9711795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIF_io(x, f, b, T):\n",
    "    out = np.log((f*x+b) / (f*x+b-T))\n",
    "    rate = 1/out\n",
    "    rate[out<=0] = 0\n",
    "    rate[np.isnan(rate)] = 0\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6e5f6",
   "metadata": {},
   "source": [
    "2. Confirm that your estimated input-output function from 1 is the same as this theoretical one. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7b718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0cdf452",
   "metadata": {},
   "source": [
    "3. Simulate a network of 10 neurons with random biases and input weights, and by plotting the estimated input-output curves with the theoretical ones, confirm that this works for a range of values. [1 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44122a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc324c0d",
   "metadata": {},
   "source": [
    "### Exercise 2: Decoding the input from a good basis\n",
    "In this exercise we will adjust the input weights and biases to create a good basis set of activation functions. We will then use these to decode the input that was provided from the neural activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d247ab8",
   "metadata": {},
   "source": [
    "1. Generate 50 neurons with basis functions which span a range of x from -5 to 5 well. The functions could be randomly sampled, or evenly spaced, or as you wish. If later exercises do not work well, it might be that you did not get a good enough basis in this step, and you could also consider using a larger set of neurons. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2638c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbde8e3f",
   "metadata": {},
   "source": [
    "Consider a specific read-out of the filtered spiketrains given by $y = \\mathbf D \\mathbf r$. Here $\\mathbf D$ are some decoding weights, and $\\mathbf r = [r_1, ..., r_N]^\\top$ are the filtered spiketrains. In the NEF the optimal decoder is found which minimizes the following loss across a large set of possible inputs $x$:\n",
    "$$L = ||\\mathbf X - \\mathbf D \\mathbf R||^2_2 + \\lambda||\\mathbf D||^2_2,$$\n",
    "\n",
    "where $\\mathbf X = [x_1, ..., x_\\text{nSamples}]^\\top$ is a representative set of input samples, $\\mathbf R = [r_1, ..., r_\\text{nSamples}]^\\top$ are the resulting activities. The first term represents the error between the decoded activities and the input, and the second part is a regularizer which makes sure the decoding weights can't get arbitrarily large (and keeps the problem constrained enough to be solvable). Normally with spiking networks this would be quite expensive to estimate (as you would have to simulate all your neurons for each input), but because we have an analytical expression for the output rates this simplifies things.\n",
    "\n",
    "We  now want to find the decoding weights $\\mathbf D$ which optimizes our loss --- very machine-learnery. And indeed one way to that is to calculate the gradient of the loss with respect to $\\mathbf D$ and perform gradient descent. However, again luckily, there exists a linear regression solution to this loss, given by:\n",
    "$$\\mathbf D = \\mathbf {XR}^\\top (\\mathbf{RR}^\\top + \\lambda \\mathbf I)^{-1},$$\n",
    "where $\\mathbf I$ is the identity matrix. $\\lambda$ should be set to some small value.\n",
    "\n",
    "2. Using your set of weights defined in 1., calculate the decoder as above. Use a range of input samples between -5 and 5. Demonstrate that $\\mathbf D$ was calculated correctly by comparing $\\mathbf X$ with $\\mathbf D \\mathbf R$. [2 points]\n",
    "\n",
    "Note: a matrix inverse can be calculated with np.linalg.inv(X). Matrix multiplications can be done by X@Y or np.dot(X, Y).\n",
    "\n",
    "Note 2: $\\mathbf X$ should be of shape (nSamples), and $\\mathbf R$ should be of shape (N, nSamples), where N is the number of neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bf94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af9037a9",
   "metadata": {},
   "source": [
    "3. Demonstrate that you can use the decoder on a network of spiking neurons. Do this by stepping the input $x(t)$ from -5 to 5 with steps of 1, and keeping the signal at each step for at least 10 ms. [2 points]\n",
    "\n",
    "Note that we need to keep the stimulus still enough for the read-out and spiking dynamics to catch up properly. It is out of scope for this exercise to make this work better, but it is not a big problem in practice to fix this. For example, one could use much faster read-out and voltage dynamics --- but this then also comes at the cost of needing many, many more spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de1f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c113c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b46caa",
   "metadata": {},
   "source": [
    "### Exercise 3: Doing nonlinear computations\n",
    "Doing now a nonlinear transformation ($x = f(x)$) is next fairly simple, and follows the same principle. We now optimize $$L = ||f(\\mathbf X) - \\mathbf D \\mathbf R||^2_2 + \\lambda||\\mathbf D||^2_2$$ instead (note the functional added in), leading to a decoder given by $$\\mathbf D^f =  {f(\\mathbf X)R}^\\top (\\mathbf{RR}^\\top + \\lambda \\mathbf I)^{-1}.$$\n",
    "\n",
    "1. Choose at least 2 different nonlinear functions f(), and find their decoders. Demonstrate that they work by comparing $\\mathbf X$ with $\\mathbf D^f \\mathbf R$ and with $f(\\mathbf X)$. [2 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcca1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b00d72ae",
   "metadata": {},
   "source": [
    "2. Show that this still works in the spiking networks. [1 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc24d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9334507a",
   "metadata": {},
   "source": [
    "# Bonus assignment (no points, just for interest)\n",
    "Above we considered everything for a 1 dimensional signal $x(t)$. How would you extent everything to consider a 2D or higher input signal? What would this mean for the input-output functions and the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c2222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
